{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchcrepe\n",
    "from einops import rearrange\n",
    "from stable_audio_tools import get_pretrained_model\n",
    "from stable_audio_tools.inference.generation import generate_diffusion_cond\n",
    "\n",
    "import numpy as np\n",
    "import pretty_midi\n",
    "import sounddevice as sd\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "import soundfile as sf\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "# Custom Helpers\n",
    "from audio_helpers import get_github_audio, plot_pitch, plot_pitch_comparison, animate_pitch_arrays, create_gradio_interface\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"Using {}\".format(device))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8b6ddef3aa76ce02",
   "metadata": {},
   "source": [
    "@torch.enable_grad()\n",
    "def calculate_pitch(audio, sample_rate):\n",
    "    # Compute pitch\n",
    "    if isinstance(audio, np.ndarray):\n",
    "        audio = torch.tensor(audio, device=device, dtype=torch.float32)\n",
    "\n",
    "    if audio.ndim > 1:\n",
    "        audio = audio.to(device=device, dtype=torch.float32).mean(dim=0, keepdim=True)\n",
    "\n",
    "    hop = int(sample_rate / 200.)  # 5 ms hop\n",
    "    pitch, periodicity = torchcrepe.predict(audio, sample_rate, hop_length=hop, fmin=50, fmax=550,\n",
    "                            model='tiny', # or 'full'\n",
    "                            batch_size=2048, device=device, return_periodicity=True, decoder=torchcrepe.decode.soft_argmax, differentiable=True)\n",
    "    # Clean up pitch\n",
    "    win_l = 3\n",
    "    periodicity = torchcrepe.filter.median(periodicity, win_l)\n",
    "    periodicity = torchcrepe.threshold.Silence(-60.)(periodicity, audio, sample_rate, hop)\n",
    "    pitch = torchcrepe.threshold.At(.5)(pitch, periodicity)\n",
    "    pitch = torchcrepe.filter.mean(pitch, win_l)\n",
    "    return pitch"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d068395ba8477eae",
   "metadata": {},
   "source": [
    "# Download model | Stable Audio Open Small\n",
    "# `https://huggingface.co/stabilityai/stable-audio-open-small`\n",
    "model, model_config = get_pretrained_model(\"stabilityai/stable-audio-open-small\")\n",
    "sample_rate = model_config[\"sample_rate\"]\n",
    "sample_size = model_config[\"sample_size\"]\n",
    "\n",
    "model = model.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8efcc4f59026ee8f",
   "metadata": {},
   "source": [
    "# if you don't have the audio file download it\n",
    "get_github_audio(\"https://github.com/pdx-cs-sound/wavs/raw/refs/heads/main/gc.wav\")\n",
    "\n",
    "target_audio, target_sr = torchaudio.load('data/audio/gc.wav')\n",
    "if sample_rate != target_sr: # Resample to model rate\n",
    "    resampler = torchaudio.transforms.Resample(sample_rate, target_sr)\n",
    "    target_audio = resampler(target_audio)\n",
    "\n",
    "# Reduce to this really specific time that stable audio open small has\n",
    "time_sec = 11.888616780045352\n",
    "target_audio = target_audio[:, :int(time_sec * sample_rate)]\n",
    "target_pitch = calculate_pitch(target_audio, sample_rate)\n",
    "\n",
    "print(f\"Target length is: {target_audio.shape[1] / sample_rate}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d16761e477d7ad10",
   "metadata": {},
   "source": [
    "from functools import partial\n",
    "\n",
    "pitch_array = []\n",
    "audio_array = []\n",
    "\n",
    "def pitch_callback(autoencoder, target_pitch, base_step_scale, alpha, in_dict):\n",
    "    x, denoised, t = in_dict['x'], in_dict['denoised'].detach(), in_dict['t'].detach().float()\n",
    "    \n",
    "    with torch.enable_grad():\n",
    "        denoised.requires_grad = True\n",
    "        # print(f\"t = {in_dict['t']:.3f}, denoised..shape, .requires_grad = {denoised.shape},, {denoised.requires_grad}\")\n",
    "\n",
    "        # PnP-Flow schedule: (1 - t)^alpha\n",
    "        time_weight = (1.0 - t) ** alpha\n",
    "        step_scale  = base_step_scale * time_weight\n",
    "        # print(f\"time_weight = {float(time_weight):.4f}, step_scale = {float(step_scale):.4f}\")\n",
    "\n",
    "        audio = autoencoder.decoder(denoised.half())\n",
    "        # print(f\"Decoder Audio \\nshape: {audio.shape} \\ndtype: {audio.dtype}\")\n",
    "        audio = audio.float()\n",
    "        # print(f\"after .float() Audio \\nshape: {audio.shape} \\ndtype: {audio.dtype}\")\n",
    "        audio = rearrange(audio, \"b d n -> d (b n)\")\n",
    "        print(\"Generated Audio shape:\", audio.shape, f\"Generated Audio length: {(audio.shape[1]/sample_rate):.2f}\")\n",
    "\n",
    "        # Display audio at each step\n",
    "        diplay_audio = audio.div(torch.max(torch.abs(audio))).clamp(-1, 1).mul(32767).to(torch.int16).cpu() # how stable audio converted it\n",
    "        # display(Audio(diplay_audio.numpy(), rate=sample_rate))\n",
    "\n",
    "        # Compute pitch\n",
    "        audio = audio.mean(dim=0, keepdim=True)\n",
    "        pitch = calculate_pitch(audio, sample_rate)\n",
    "\n",
    "        # Graph both pitch\n",
    "        pitch_array.append(pitch.detach().cpu().numpy()) # make sure to use this in jupter for external context\n",
    "        audio_array.append(diplay_audio.detach().cpu().numpy())\n",
    "        # plot_pitch_comparison(pitch, target_pitch, sample_rate, overlay=True)\n",
    "\n",
    "        loss_fn = MSELoss()\n",
    "        loss = loss_fn(torch.nan_to_num(pitch, nan=0.),\n",
    "                        torch.nan_to_num(target_pitch, nan=0.))\n",
    "        # print(\"loss.requires_grad =\",loss.requires_grad)\n",
    "\n",
    "        grad_x = torch.autograd.grad(loss, denoised, grad_outputs=torch.ones_like(loss), retain_graph=False, allow_unused=True)[0]\n",
    "        # d_denoised = -step_scale * grad_x\n",
    "\n",
    "        # Clip and update\n",
    "        with torch.no_grad():\n",
    "            if grad_x.norm() > 0.1:\n",
    "                grad_x = grad_x * (0.1 / grad_x.norm())\n",
    "            denoised += step_scale * grad_x\n",
    "            denoised.requires_grad_(False)\n",
    "\n",
    "            print(f\"loss={loss.item():.6f}\")\n",
    "    return"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def pitch_guidance_callback(autoencoder, target_pitch, strength, in_dict):\n",
    "    \"Main guidance routine using chroma features\"\n",
    "    t, x, denoised = in_dict['t'], in_dict['x'], in_dict['denoised']\n",
    "    if target_pitch is None:\n",
    "        print(f\"t = {t:.3f}: target_pitch is None. Skipping pitch guidance callback\")\n",
    "        return\n",
    "    inner_strength = strength / 10  # or /5, adjust based on testing\n",
    "    print('--')\n",
    "    with torch.enable_grad():\n",
    "        for step in range(10):\n",
    "            denoised.requires_grad_(True)  # Enable grad on denoised itself\n",
    "            pred_audio = autoencoder.decoder(denoised.half())\n",
    "            shape_save = pred_audio.shape[-1]\n",
    "            kernel_size = 64\n",
    "            pred_audio = torch.nn.functional.avg_pool1d(\n",
    "                pred_audio, kernel_size=kernel_size, stride=1, padding=kernel_size//2\n",
    "            )\n",
    "            pred_audio = pred_audio[..., :shape_save]\n",
    "            #print(\"pred_audio.shape =\",pred_audio.shape)\n",
    "            # pred_audio = torchaudio.functional.lowpass_biquad(\n",
    "            #     pred_audio, sample_rate=sample_rate, cutoff_freq=4000)\n",
    "            pred_audio = rearrange(pred_audio, \"b d n -> d (b n)\")\n",
    "            pred_pitch = calculate_pitch(pred_audio, sample_rate)\n",
    "\n",
    "            loss = F.mse_loss(\n",
    "                torch.nan_to_num(pred_pitch, nan=0),\n",
    "                torch.nan_to_num(target_pitch, nan=0)\n",
    "                )\n",
    "            grad = torch.autograd.grad(loss, denoised)[0]\n",
    "            #gamma_t = (1 - t)\n",
    "            gamma_t = 0.3 + 0.7 * (1 - t)  # Goes from 0.3 â†’ 1.0\n",
    "\n",
    "            # Clip and update\n",
    "            with torch.no_grad():\n",
    "                if grad.norm() > 0.1:\n",
    "                    grad = grad * (0.1 / grad.norm())\n",
    "                denoised -= inner_strength * gamma_t * grad\n",
    "                denoised.requires_grad_(False)\n",
    "\n",
    "            print(f\"  Inner step {step}: loss={loss.item():.6f}\")\n",
    "\n"
   ],
   "id": "86cd49c9ea586a56",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d6069e2d26586838",
   "metadata": {},
   "source": [
    "conditioning = [{\n",
    "    \"prompt\": \"nylon guitar country sound\",  # This prompt is quite bad on small, but small does work\n",
    "    \"seconds_total\": time_sec\n",
    "}]\n",
    "pitch_array = []\n",
    "audio_array = []\n",
    "\n",
    "autoencoder = model._modules['pretransform']._modules.get(\"model\")\n",
    "# Params\n",
    "base_step_scale = 1\n",
    "alpha = 5\n",
    "callback_wrapper = partial(pitch_callback, autoencoder, target_pitch, base_step_scale, alpha)\n",
    "\n",
    "# callback_wrapper = partial(pitch_guidance_callback, autoencoder, target_pitch, 50)\n",
    "\n",
    "# Generate stereo audio\n",
    "output = generate_diffusion_cond(\n",
    "    model,\n",
    "    # Marco's Notes:\n",
    "    # 7 steps works good for sao small, higher than that gets scary\n",
    "    # If using normal sao higher steps is usually pretty good.\n",
    "    conditioning=conditioning,\n",
    "    steps=50,\n",
    "    cfg_scale=1., # Config of 1 often good for small, higher works on normal\n",
    "    sample_size=sample_size,\n",
    "    sigma_min=10,\n",
    "    sigma_max=300,\n",
    "    # sampler_type=\"dpmpp-3m-sde\",  # Use this for normal open\n",
    "    sampler_type=\"pingpong\",  # Use this for small\n",
    "    device=device,\n",
    "    callback=callback_wrapper\n",
    ")\n",
    "\n",
    "# Rearrange audio batch to a single sequence\n",
    "output = rearrange(output, \"b d n -> d (b n)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Now you can hear the final audio",
   "id": "2e5161cca3e0618"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Peak normalize, convert to int16\n",
    "cleaned_output = output.to(torch.float32).div(torch.max(torch.abs(output))).clamp(-1, 1).mul(32767).to(torch.int16).cpu()\n",
    "\n",
    "# Clip length\n",
    "#clipped_output = cleaned_output[..., :int(sample_rate * total_seconds)]\n",
    "\n",
    "Audio(cleaned_output.numpy(), rate=sample_rate)"
   ],
   "id": "7aba34dc2899a136",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Sounds like garbage right? (well thats if you took over like 10 steps)\n",
    "Instead lets run gradio and try to see what some of the audio looked like during the journey"
   ],
   "id": "7dfded6953c3365f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "demo = create_gradio_interface(pitch_array, target_pitch, sample_rate, audio_array, target_audio)",
   "id": "eba91c788b91ab39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "demo.launch()",
   "id": "e7179cac50ab76a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "demo.close()",
   "id": "67b32d302eda53ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# use this to close any still open ones\n",
    "import gradio as gr\n",
    "gr.close_all()"
   ],
   "id": "780262049d5432f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "You can also view just the pitch frames overlaid with the `target_pitch` and then select the audio frame in the next cell",
   "id": "85941118e5d76363"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "animate_pitch_arrays(pitch_array, sample_rate, target_pitch)",
   "id": "1e0eaaa7ecc69d36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "frame_number = 35\n",
    "Audio(audio_array[frame_number], rate=sample_rate)"
   ],
   "id": "87b854e7b2a4fc8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If you want to compare to the original audio run this",
   "id": "d93dc89cd881b1f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Also display original audio\n",
    "Audio(target_audio.numpy(), rate=sample_rate)"
   ],
   "id": "9788093827cb5d2d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
