{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T14:41:27.551694Z",
     "start_time": "2025-11-12T14:41:20.970581Z"
    }
   },
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchcrepe\n",
    "from einops import rearrange\n",
    "from stable_audio_tools import get_pretrained_model\n",
    "from stable_audio_tools.inference.generation import generate_diffusion_cond\n",
    "\n",
    "import numpy as np\n",
    "import pretty_midi\n",
    "import sounddevice as sd\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "import soundfile as sf\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "# Custom Helpers\n",
    "from audio_helpers import render_midi, get_github_audio\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"Using {}\".format(device))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached SoundFont: soundfonts/FluidR3Mono_GM.sf3\n",
      "Using mps\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "8b6ddef3aa76ce02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T14:41:27.569235Z",
     "start_time": "2025-11-12T14:41:27.564954Z"
    }
   },
   "source": [
    "@torch.enable_grad()\n",
    "def calculate_pitch(audio, sample_rate):\n",
    "    # Compute pitch\n",
    "    audio = audio.mean(dim=0, keepdim=True).to(device=device, dtype=torch.float32) # Down from stereo to mono\n",
    "    hop = int(sample_rate / 200.)  # 5 ms hop\n",
    "    pitch, periodicity = torchcrepe.predict(audio, sample_rate, hop_length=hop, fmin=50, fmax=550,\n",
    "                            model='tiny', # or 'full'\n",
    "                            batch_size=2048, device=device, return_periodicity=True, decoder=torchcrepe.decode.soft_argmax, differentiable=True)\n",
    "    # Clean up pitch\n",
    "    win_l = 3\n",
    "    periodicity = torchcrepe.filter.median(periodicity, win_l)\n",
    "    periodicity = torchcrepe.threshold.Silence(-60.)(periodicity, audio, sample_rate, hop)\n",
    "    pitch = torchcrepe.threshold.At(.5)(pitch, periodicity)\n",
    "    pitch = torchcrepe.filter.mean(pitch, win_l)\n",
    "    return pitch"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "d068395ba8477eae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T14:41:34.957168Z",
     "start_time": "2025-11-12T14:41:27.578680Z"
    }
   },
   "source": [
    "# Download model | Stable Audio Open Small\n",
    "# `https://huggingface.co/stabilityai/stable-audio-open-small`\n",
    "model, model_config = get_pretrained_model(\"stabilityai/stable-audio-open-small\")\n",
    "sample_rate = model_config[\"sample_rate\"]\n",
    "sample_size = model_config[\"sample_size\"]\n",
    "\n",
    "model = model.to(device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'flash_attn'\n",
      "flash_attn not installed, disabling Flash Attention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcocassar/Projects/DLAIE/self/temp/sao-guidance/.venv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "8efcc4f59026ee8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T14:41:36.272338Z",
     "start_time": "2025-11-12T14:41:35.012555Z"
    }
   },
   "source": [
    "# if you don't have the audio file download it\n",
    "get_github_audio(\"https://github.com/pdx-cs-sound/wavs/raw/refs/heads/main/gc.wav\")\n",
    "\n",
    "target_audio, target_sr = torchaudio.load('data/audio/gc.wav')\n",
    "if sample_rate != target_sr: # Resample to model rate\n",
    "    resampler = torchaudio.transforms.Resample(sample_rate, target_sr)\n",
    "    target_audio = resampler(target_audio)\n",
    "\n",
    "# Reduce to this really specific time that stable audio open small has\n",
    "time_sec = 11.888616780045352\n",
    "target_audio = target_audio[:, :int(time_sec * sample_rate)]\n",
    "target_pitch = calculate_pitch(target_audio, sample_rate)\n",
    "\n",
    "print(f\"Target length is: {target_audio.shape[1] / sample_rate}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached file: data/audio/gc.wav\n",
      "Target length is: 11.888616780045352\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "d16761e477d7ad10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T14:42:58.159651Z",
     "start_time": "2025-11-12T14:42:58.152721Z"
    }
   },
   "source": [
    "from functools import partial\n",
    "\n",
    "def pitch_callback(model, target_pitch, base_step_scale, alpha, in_dict):\n",
    "    x, denoised, t = in_dict['x'], in_dict['denoised'], in_dict['t'].detach().float()\n",
    "    \n",
    "    with torch.enable_grad():\n",
    "        x.requires_grad, denoised.requires_grad = True, True\n",
    "        print(f\"t = {in_dict['t']:.3f}, denoised..shape, .requires_grad = {denoised.shape},, {denoised.requires_grad}\")\n",
    "\n",
    "        # PnP-Flow schedule: (1 - t)^alpha\n",
    "        time_weight = (1.0 - t) ** alpha\n",
    "        step_scale  = base_step_scale * time_weight\n",
    "        print(f\"time_weight = {float(time_weight):.4f}, step_scale = {float(step_scale):.4f}\")\n",
    "\n",
    "        autoencoder = model._modules['pretransform']._modules.get(\"model\")\n",
    "\n",
    "        audio = autoencoder.decoder(denoised.half())\n",
    "        audio = rearrange(audio, \"b d n -> d (b n)\")\n",
    "        print(\"Generated Audio shape:\", audio.shape, f\"Generated Audio length: {(audio.shape[1]/sample_rate):.2f}\")\n",
    "\n",
    "        # Display audio at each step\n",
    "        diplay_audio = audio.to(torch.float32).div(torch.max(torch.abs(audio))).clamp(-1, 1).mul(32767).to(torch.int16).cpu()\n",
    "        display(Audio(diplay_audio.numpy(), rate=sample_rate))\n",
    "        \n",
    "        # Compute pitch\n",
    "        audio = audio.mean(dim=0, keepdim=True).to(device=device, dtype=torch.float32) # Down from stereo to mono\n",
    "        pitch = calculate_pitch(audio, sample_rate)\n",
    "        print(\"pitch.requires_grad =\",pitch.requires_grad)\n",
    "        print(\"pitch.shape =\",pitch.shape)\n",
    "\n",
    "        loss_fn = MSELoss()\n",
    "        loss = loss_fn(pitch, target_pitch)\n",
    "        print(\"loss.requires_grad =\",loss.requires_grad)\n",
    "\n",
    "        grad_x = torch.autograd.grad(loss, denoised, grad_outputs=torch.ones_like(loss), retain_graph=False, allow_unused=True)[0]\n",
    "        d_denoised = -step_scale * grad_x\n",
    "        denoised = denoised + d_denoised\n",
    "\n",
    "    in_dict['denoised'] = denoised\n",
    "    in_dict['x'] = x\n",
    "    return\n",
    "\n",
    "# Params\n",
    "base_step_scale = .5\n",
    "alpha = 1\n",
    "\n",
    "callback_wrapper = partial(pitch_callback, model, target_pitch, base_step_scale, alpha)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "d6069e2d26586838",
   "metadata": {},
   "source": [
    "conditioning = [{\n",
    "    \"prompt\": \"nylon guitar country scale\",  # This prompt is quite bad on small, but small does work\n",
    "    \"seconds_total\": time_sec\n",
    "}]\n",
    "\n",
    "# Generate stereo audio\n",
    "output = generate_diffusion_cond(\n",
    "    model,\n",
    "    # Marco's Notes:\n",
    "    # 7 steps works good for sao small, higher than that gets scary\n",
    "    # If using normal sao higher steps is usually pretty good.\n",
    "    conditioning=conditioning,\n",
    "    steps=13,\n",
    "    cfg_scale=1, # Config of 1 often good for small, higher works on normal\n",
    "    sample_size=sample_size,\n",
    "    sigma_min=10,\n",
    "    sigma_max=300,\n",
    "    # sampler_type=\"dpmpp-3m-sde\",  # Use this for normal open\n",
    "    sampler_type=\"pingpong\",  # Use this for small\n",
    "    device=device,\n",
    "    callback=callback_wrapper\n",
    ")\n",
    "\n",
    "# Rearrange audio batch to a single sequence\n",
    "output = rearrange(output, \"b d n -> d (b n)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "afc2511e528de8ed",
   "metadata": {},
   "source": [
    "# Peak normalize, convert to int16\n",
    "cleaned_output = output.to(torch.float32).div(torch.max(torch.abs(output))).clamp(-1, 1).mul(32767).to(torch.int16).cpu()\n",
    "\n",
    "# Clip length\n",
    "#clipped_output = cleaned_output[..., :int(sample_rate * total_seconds)]\n",
    "\n",
    "Audio(cleaned_output.numpy(), rate=sample_rate)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "db0fa922ed7d4a1a",
   "metadata": {},
   "source": [
    "# Also display original audio\n",
    "Audio(target_audio.numpy(), rate=sample_rate)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dabccef-775e-4ed8-99b2-b867f3abdf48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
