{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Imports\n",
    "import torchcrepe\n",
    "import numpy as np\n",
    "import pretty_midi\n",
    "import sounddevice as sd\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "import soundfile as sf\n",
    "\n",
    "# Custom Helpers\n",
    "from audio_helpers import render_midi"
   ],
   "id": "b4a00fca685b5567",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "audio, sr = torchcrepe.load.audio(\"./data/BDCT-0/4YNW3G/Audio Files/Nord.02 L.08_01.wav\")\n",
    "print(f\"Before: {audio.shape}\")\n",
    "audio = audio.mean(dim=0, keepdim=True)\n",
    "print(f\"After: {audio.shape}\")"
   ],
   "id": "df6d7d5190739f8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "audio, sr = torchcrepe.load.audio(\"./local/A melody ig copy.m4a\")",
   "id": "d845da9623e883a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import essentia\n",
    "import essentia.standard as es\n",
    "audiofile = \"./local/A melody ig copy.m4a\"\n",
    "loader = es.EqloudLoader(filename=audiofile, sampleRate=sr)\n",
    "audio = loader()\n",
    "print(\"Duration of the audio sample [sec]:\")\n",
    "print(len(audio)/sr)"
   ],
   "id": "a4c20cb86bc1e614",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# For embedding audio player\n",
    "import IPython\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import plot, show, figure, imshow\n",
    "plt.rcParams['figure.figsize'] = (15, 6)\n",
    "\n",
    "import numpy"
   ],
   "id": "f2c427c7c796786d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract the pitch curve\n",
    "# PitchMelodia takes the entire audio signal as input (no frame-wise processing is required).\n",
    "\n",
    "pitch_extractor = es.PredominantPitchMelodia(frameSize=2048, hopSize=128)\n",
    "pitch_values, pitch_confidence = pitch_extractor(audio)\n",
    "\n",
    "# Pitch is estimated on frames. Compute frame time positions.\n",
    "pitch_times = numpy.linspace(0.0,len(audio)/44100.0,len(pitch_values) )\n",
    "\n",
    "# Plot the estimated pitch contour and confidence over time.\n",
    "f, axarr = plt.subplots(2, sharex=True)\n",
    "axarr[0].plot(pitch_times, pitch_values)\n",
    "axarr[0].set_title('estimated pitch [Hz]')\n",
    "axarr[1].plot(pitch_times, pitch_confidence)\n",
    "axarr[1].set_title('pitch confidence')\n",
    "plt.show()"
   ],
   "id": "ef76c0bb9a428425",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "IPython.display.Audio(audio, rate=sr)",
   "id": "13dd8012c2f2c48b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "audio.shape",
   "id": "5e34b782788c98cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from mir_eval.sonify import pitch_contour\n",
    "\n",
    "from tempfile import TemporaryDirectory\n",
    "temp_dir = TemporaryDirectory()\n",
    "\n",
    "# Essentia operates with float32 ndarrays instead of float64, so let's cast it.\n",
    "synthesized_melody = pitch_contour(pitch_times, pitch_values, 44100).astype(numpy.float32)[:len(audio)]\n",
    "es.AudioWriter(filename=temp_dir.name + 'piano_test.mp3', format='mp3')(es.StereoMuxer()(audio, synthesized_melody))\n",
    "\n",
    "IPython.display.Audio(temp_dir.name + 'piano_test.mp3')"
   ],
   "id": "e48c757ac6a7ab62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from essentia.standard import KeyExtractor\n",
    "\n",
    "key_extractor = KeyExtractor()\n",
    "key, scale, strength = key_extractor(audio)\n",
    "\n",
    "print(f\"Key: {key}, Scale: {scale}, Confidence: {strength:.3f}\")"
   ],
   "id": "579b622a19d4bb48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "y, sr = librosa.load(\"./data/BDCT-0/4YNW3G/Audio Files/Bass.08_01.wav\")\n",
    "chroma = librosa.feature.chroma_cqt(y=y, sr=sr)\n",
    "chroma_mean = np.mean(chroma, axis=1)\n",
    "\n",
    "# Find key estimate\n",
    "key, mode = mir_eval.key\n",
    "print(f\"Estimated key: {key} {mode}\")"
   ],
   "id": "6d57b8399cde5759",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load audio\n",
    "# audio, sr = torchcrepe.load.audio(\"./data/BDCT-0/4YNW3G/Audio Files/Bass.08_01.wav\")\n",
    "audio, sr = torchcrepe.load.audio(\"./local/A melody ig copy.m4a\")\n",
    "print(f\"Before: {audio.shape}\")\n",
    "audio = audio.mean(dim=0, keepdim=True) # for if there is 2\n",
    "print(f\"After: {audio.shape}\")\n",
    "\n",
    "# Here we'll use a 5 millisecond hop length\n",
    "hop_length = int(sr / 200.)\n",
    "\n",
    "# Provide a sensible frequency range for your domain (upper limit is 2006 Hz)\n",
    "# This would be a reasonable range for speech\n",
    "fmin = 50\n",
    "fmax = 550\n",
    "\n",
    "# Select a model capacity--one of \"tiny\" or \"full\"\n",
    "model = 'tiny'\n",
    "\n",
    "# Choose a device to use for inference\n",
    "device = 'mps:0'\n",
    "\n",
    "# Pick a batch size that doesn't cause memory errors on your gpu\n",
    "batch_size = 2048\n",
    "\n",
    "# Compute pitch using first gpu\n",
    "pitch, periodicity = torchcrepe.predict(audio,\n",
    "                           sr,\n",
    "                           hop_length,\n",
    "                           fmin,\n",
    "                           fmax,\n",
    "                           model,\n",
    "                           batch_size=batch_size,\n",
    "                           device=device, return_periodicity=True)"
   ],
   "id": "871874946af4da66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# We'll use a 15 millisecond window assuming a hop length of 5 milliseconds\n",
    "win_length = 3\n",
    "\n",
    "# Median filter noisy confidence value\n",
    "new_periodicity = torchcrepe.filter.median(periodicity, win_length)\n",
    "new_periodicity = torchcrepe.threshold.Silence(-60.)(new_periodicity,\n",
    "                                                 audio,\n",
    "                                                 sr,\n",
    "                                                 hop_length)\n",
    "\n",
    "# Remove inharmonic regions\n",
    "new_pitch = torchcrepe.threshold.At(.5)(pitch, new_periodicity)\n",
    "\n",
    "# Optionally smooth pitch to remove quantization artifacts\n",
    "new_pitch = torchcrepe.filter.mean(new_pitch, win_length)"
   ],
   "id": "6386dc4b393ba8eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8066e90977d3ffe8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "#deciding a pitch",
   "id": "cb53184a6eb7ecff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# convert to np\n",
    "pitch_hz = new_pitch.squeeze().cpu().numpy()\n",
    "\n",
    "crepe_pm = pretty_midi.PrettyMIDI()\n",
    "instrument = pretty_midi.Instrument(program=0)\n",
    "frame_time = hop_length / sr\n",
    "\n",
    "# filter negatives\n",
    "threshold = 0.5\n",
    "voiced = pitch_hz > 0\n",
    "midi_values = np.full_like(pitch_hz, np.nan, dtype=float)\n",
    "midi_values[voiced] = librosa.hz_to_midi(pitch_hz[voiced])\n",
    "\n",
    "current_note = None\n",
    "start_time = 0.0\n",
    "\n",
    "for i, v in enumerate(midi_values):\n",
    "    if voiced[i]:\n",
    "        if current_note is None: # handle new note\n",
    "            current_note = int(round(v))\n",
    "            start_time = i * frame_time\n",
    "        elif abs(v - current_note) > threshold:\n",
    "            end_time = i * frame_time\n",
    "            instrument.notes.append(\n",
    "                pretty_midi.Note(velocity=100, pitch=current_note, start=start_time, end=end_time))\n",
    "            current_note = int(round(v))\n",
    "            start_time = i * frame_time\n",
    "    else:\n",
    "        if current_note is not None:\n",
    "            # end note if silence\n",
    "            end_time = i * frame_time\n",
    "            instrument.notes.append(\n",
    "                pretty_midi.Note(velocity=100, pitch=current_note, start=start_time, end=end_time))\n",
    "            current_note = None\n",
    "\n",
    "# close last note\n",
    "if current_note is not None:\n",
    "    end_time = len(pitch_hz) * frame_time\n",
    "    instrument.notes.append(\n",
    "        pretty_midi.Note(velocity=100, pitch=current_note, start=start_time, end=end_time))\n",
    "\n",
    "crepe_pm.instruments.append(instrument)"
   ],
   "id": "d83156873e13d507",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "\n",
    "# Convert to numpy\n",
    "pitch_hz = new_pitch.squeeze().cpu().numpy()\n",
    "\n",
    "crepe_pm = pretty_midi.PrettyMIDI()\n",
    "instrument = pretty_midi.Instrument(program=0)\n",
    "frame_time = hop_length / sr\n",
    "\n",
    "# Filter negatives and convert to MIDI\n",
    "voiced = pitch_hz > 0\n",
    "midi_values = np.full_like(pitch_hz, np.nan, dtype=float)\n",
    "midi_values[voiced] = librosa.hz_to_midi(pitch_hz[voiced])\n",
    "\n",
    "# Apply additional smoothing to MIDI values to reduce jitter\n",
    "# Use a larger window for more stable pitch tracking\n",
    "smooth_window = 9  # 45ms at 5ms hop\n",
    "midi_smoothed = np.full_like(midi_values, np.nan)\n",
    "valid_mask = ~np.isnan(midi_values)\n",
    "if np.any(valid_mask):\n",
    "    # Only smooth valid (voiced) regions\n",
    "    midi_smoothed[valid_mask] = scipy.signal.medfilt(midi_values[valid_mask], smooth_window)\n",
    "\n",
    "# Parameters for note segmentation\n",
    "pitch_threshold = 1.5  # Increased from 0.5 - more tolerant of pitch variation\n",
    "min_note_duration = 0.05  # 50ms minimum note duration\n",
    "min_gap_duration = 0.03  # 30ms minimum gap to split notes\n",
    "\n",
    "# Improved note segmentation with hysteresis\n",
    "notes = []\n",
    "current_note = None\n",
    "start_time = None\n",
    "note_pitches = []  # Collect pitches for averaging\n",
    "\n",
    "for i, midi_val in enumerate(midi_smoothed):\n",
    "    time = i * frame_time\n",
    "\n",
    "    if not np.isnan(midi_val):  # Voiced frame\n",
    "        if current_note is None:\n",
    "            # Start new note\n",
    "            current_note = midi_val\n",
    "            start_time = time\n",
    "            note_pitches = [midi_val]\n",
    "        else:\n",
    "            # Check if pitch changed significantly\n",
    "            median_pitch = np.median(note_pitches)\n",
    "            if abs(midi_val - median_pitch) > pitch_threshold:\n",
    "                # End current note if it's long enough\n",
    "                if time - start_time >= min_note_duration:\n",
    "                    notes.append({\n",
    "                        'pitch': int(round(median_pitch)),\n",
    "                        'start': start_time,\n",
    "                        'end': time,\n",
    "                        'velocity': 100\n",
    "                    })\n",
    "                # Start new note\n",
    "                current_note = midi_val\n",
    "                start_time = time\n",
    "                note_pitches = [midi_val]\n",
    "            else:\n",
    "                # Continue current note\n",
    "                note_pitches.append(midi_val)\n",
    "    else:  # Unvoiced frame\n",
    "        if current_note is not None:\n",
    "            # Check if gap is long enough to end note\n",
    "            # Look ahead to see if voice returns soon\n",
    "            look_ahead = min(i + int(min_gap_duration / frame_time), len(midi_smoothed) - 1)\n",
    "            if np.all(np.isnan(midi_smoothed[i:look_ahead])):\n",
    "                # Gap is long enough, end note\n",
    "                if time - start_time >= min_note_duration:\n",
    "                    median_pitch = np.median(note_pitches)\n",
    "                    notes.append({\n",
    "                        'pitch': int(round(median_pitch)),\n",
    "                        'start': start_time,\n",
    "                        'end': time,\n",
    "                        'velocity': 100\n",
    "                    })\n",
    "                current_note = None\n",
    "                note_pitches = []\n",
    "\n",
    "# Close last note if exists\n",
    "if current_note is not None and start_time is not None:\n",
    "    time = len(midi_smoothed) * frame_time\n",
    "    if time - start_time >= min_note_duration:\n",
    "        median_pitch = np.median(note_pitches)\n",
    "        notes.append({\n",
    "            'pitch': int(round(median_pitch)),\n",
    "            'start': start_time,\n",
    "            'end': time,\n",
    "            'velocity': 100\n",
    "        })\n",
    "\n",
    "# Add notes to instrument\n",
    "for note in notes:\n",
    "    instrument.notes.append(\n",
    "        pretty_midi.Note(\n",
    "            velocity=note['velocity'],\n",
    "            pitch=note['pitch'],\n",
    "            start=note['start'],\n",
    "            end=note['end']\n",
    "        )\n",
    "    )\n",
    "\n",
    "crepe_pm.instruments.append(instrument)"
   ],
   "id": "c3c8139a13e9f4b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "render_midi(crepe_pm)",
   "id": "42ab5a891bb21552",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gen_audio = generate_audio_midi(crepe_pm, 16000)\n",
    "sf.write(\"Bass.08_01_pitch_to_note.wav\", gen_audio, 16000)"
   ],
   "id": "6c8aec761788087b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming:\n",
    "# audio: 1D NumPy array or tensor\n",
    "# pitch: 1D NumPy array of f0 in Hz\n",
    "# sr, hop_length defined\n",
    "\n",
    "cleaned_audio = audio.squeeze().cpu().numpy()\n",
    "cleaned_pitch = pitch.squeeze().cpu().numpy()\n",
    "times = np.arange(len(pitch)) * hop_length / sr\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "librosa.display.waveshow(cleaned_audio, sr=sr, alpha=0.6, color='gray')\n",
    "plt.plot(times, cleaned_pitch / np.max(cleaned_pitch) * np.max(cleaned_audio), color='deepskyblue', linewidth=2)\n",
    "plt.title(\"Waveform + Normalized Pitch Contour\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "33c832e78f585a42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "pitch_hz = pitch.squeeze().cpu().numpy()\n",
    "# pitch_hz = pitch\n",
    "\n",
    "# voiced mask: only frames with real pitch\n",
    "voiced = pitch_hz > 0\n",
    "\n",
    "midi = np.empty_like(pitch_hz, dtype=float)\n",
    "midi[:] = np.nan\n",
    "midi[voiced] = librosa.hz_to_midi(pitch_hz[voiced])\n",
    "\n",
    "# convert only voiced frames to note names\n",
    "note_names = np.empty_like(pitch_hz, dtype=object)\n",
    "note_names[:] = None\n",
    "note_names[voiced] = librosa.midi_to_note(midi[voiced])\n",
    "\n",
    "# e.g. filter out silence if you just want a list of notes\n",
    "notes_compact = librosa.midi_to_note(midi[voiced])"
   ],
   "id": "cae9646848e39978",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "notes_compact",
   "id": "378583cd429057b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "dfe0eed2e97b7220",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8b44ea33dd51a27a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
