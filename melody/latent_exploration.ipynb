{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from einops import rearrange\n",
    "from stable_audio_tools import get_pretrained_model\n",
    "from stable_audio_tools.inference.generation import generate_diffusion_cond\n",
    "from tqdm.notebook import tqdm\n",
    "from AudacityHelper import AudacityPipeline\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else \"cpu\"\n",
    "print(\"Using {}\".format(device))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Encode Thy Latents",
   "id": "277b5459615391b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Choose your model! (reading this mentally with an announcer voice) Could use normal but we use small",
   "id": "b5c4f5ba7f264bfd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Download model | Stable Audio Open (small)\n",
    "# `https://huggingface.co/stabilityai/stable-audio-open-small`\n",
    "model, model_config = get_pretrained_model(\"stabilityai/stable-audio-open-small\")\n",
    "# model, model_config = get_pretrained_model(\"stabilityai/stable-audio-open-1.0\")\n",
    "sample_rate = model_config[\"sample_rate\"]\n",
    "sample_size = model_config[\"sample_size\"]\n",
    "\n",
    "model = model.to(device).eval()"
   ],
   "id": "a39f1bc0fb028ee9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# I have to use this to handle bad audiofiles ugh | to use this you have to have audacity open and have to enable scripting. (Ask GPT its pretty easy to do just a little finicky)\n",
    "ap = AudacityPipeline()"
   ],
   "id": "a366aff603e21ceb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_autoencoder(model):\n",
    "    return model._modules['pretransform']._modules.get(\"model\")\n",
    "\n",
    "autoencoder = get_autoencoder(model).to(device)\n",
    "\n",
    "sample_param = next(autoencoder.parameters())\n",
    "audio_device = sample_param.device\n",
    "audio_dtype = sample_param.dtype\n",
    "print(f\"Using {audio_device} device {audio_dtype} dtype\")\n",
    "\n",
    "def clean_audio_dim(audio, debug=False):\n",
    "    if audio.dim() == 1: audio = audio.unsqueeze(0).unsqueeze(0)\n",
    "    if audio.dim() == 2: audio = audio.unsqueeze(0)\n",
    "    if audio.shape[1] == 1: audio = audio.repeat(1, 2, 1)\n",
    "    audio = audio.to(device=audio_device, dtype=audio_dtype)\n",
    "    if debug: print(f\"Shape: {audio.shape} \\n Device: {audio.device}\")\n",
    "    return audio\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_audio_latent(path_to_audio, autoencoder):\n",
    "    audio, audio_sr = torchaudio.load(path_to_audio)\n",
    "    audio = clean_audio_dim(audio)\n",
    "    latents = autoencoder.encode(audio)\n",
    "    return latents\n",
    "\n",
    "def encode_audio_latents(list_of_audio_paths, autoencoder, save_to='data/audio_latents'):\n",
    "    save_dir = os.path.abspath(save_to)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    errored_paths = []\n",
    "    error_log = ''\n",
    "    for path_to_audio in tqdm(list_of_audio_paths):\n",
    "        audio_name = os.path.splitext(os.path.basename(path_to_audio))[0]\n",
    "        try:\n",
    "            latents = encode_audio_latent(path_to_audio, autoencoder)\n",
    "            save_path = os.path.join(save_dir, f\"{audio_name}.pt\")\n",
    "            torch.save(latents.cpu(), save_path)\n",
    "        except Exception as e:\n",
    "            message = f\"Ran into error on file {audio_name}\\nAttempting to fix with audacity:\\n\"\n",
    "            error_log += f'{message}\\n'\n",
    "            print(message)\n",
    "            new_path = ap.clean_audio_via_audacity(path_to_audio)\n",
    "            if new_path:\n",
    "                try:\n",
    "                    latents = encode_audio_latent(new_path, autoencoder)\n",
    "                    save_path = os.path.join(save_dir, f\"{audio_name}.pt\")\n",
    "                    torch.save(latents.cpu(), save_path)\n",
    "                    print(\"Successfully fixed with audacity processed file:\", new_path)\n",
    "                except Exception as e:\n",
    "                    message = f\"Failed to fix with new audacity processed file: {e}\"\n",
    "                    print(message)\n",
    "                    error_log += f'{message}\\n'\n",
    "                    errored_paths.append(path_to_audio)\n",
    "            else:\n",
    "                errored_paths.append(path_to_audio)\n",
    "\n",
    "    if error_log:\n",
    "        with open(os.path.join(save_dir, 'error.log'), 'a') as f:\n",
    "            f.write(error_log)\n",
    "            if errored_paths:\n",
    "                f.write('\\nFailed files:\\n')\n",
    "                f.write('\\n'.join(errored_paths))\n",
    "            f.write('\\n\\n')\n",
    "\n",
    "def get_audio_file_paths(folder, audio_data_path = \"data/BDCT-0/\"):\n",
    "    audio_file_paths = []\n",
    "    base_path = os.path.join(os.path.abspath(audio_data_path), folder)\n",
    "    for file in os.listdir(os.path.join(base_path, 'Audio Files')):\n",
    "        audio_file_paths += [os.path.join(base_path, 'Audio Files', file)]\n",
    "\n",
    "    for file in os.listdir(os.path.join(base_path, 'Bounced Files')):\n",
    "        audio_file_paths += [os.path.join(base_path, 'Bounced Files', file)]\n",
    "\n",
    "    return audio_file_paths"
   ],
   "id": "641680b662033c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "focused_directory = 'UNVWTU'",
   "id": "e3f76a37b2fb740f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "audio_file_paths = get_audio_file_paths(focused_directory)",
   "id": "789b572b00c3ab73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "encode_audio_latents(audio_file_paths, autoencoder, save_to=f'data/audio_latents/{focused_directory}')",
   "id": "7f2578028052e5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Playground",
   "id": "ede7b958360e732c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Download model | Stable Audio Open (normal)\n",
    "# `https://huggingface.co/stabilityai/stable-audio-open-1.0`\n",
    "model, model_config = get_pretrained_model(\"stabilityai/stable-audio-open-1.0\")\n",
    "sample_rate = model_config[\"sample_rate\"]\n",
    "sample_size = model_config[\"sample_size\"]\n",
    "\n",
    "model = model.to(device)"
   ],
   "id": "7caf06426814524d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set up text and timing conditioning\n",
    "conditioning = [{\n",
    "    \"prompt\": \"60 BPM jazz saxophone solo\",  # This prompt is quite bad on small, but small does work\n",
    "    # \"seconds_start\": 0,\n",
    "    \"seconds_total\": 11\n",
    "}]\n",
    "\n",
    "# Generate stereo audio\n",
    "output = generate_diffusion_cond(\n",
    "    model,\n",
    "    steps=8,\n",
    "    cfg_scale=1.0,\n",
    "    conditioning=conditioning,\n",
    "    sample_size=sample_size,\n",
    "    # sigma_min=0.3,\n",
    "    # sigma_max=500,\n",
    "    # sampler_type=\"dpmpp-3m-sde\",  # Use this for normal open\n",
    "    sampler_type=\"pingpong\",  # Use this for small\n",
    "    device=device\n",
    ")\n",
    "\n",
    "\n",
    "# Rearrange audio batch to a single sequence\n",
    "output = rearrange(output, \"b d n -> d (b n)\")\n",
    "\n",
    "# Peak normalize, clip, convert to int16, and save to file\n",
    "output = output.to(torch.float32).div(torch.max(torch.abs(output))).clamp(-1, 1).mul(32767).to(torch.int16).cpu()"
   ],
   "id": "1b9502bde01e3cb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# output: (channels, samples) float32 on CPU, normalized safely\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(output.numpy(), rate=sample_rate))"
   ],
   "id": "f8d30dd858d7657b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ae = model._modules['pretransform']._modules.get(\"model\")",
   "id": "7965691d3a17c566",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ae",
   "id": "68e5f4b084c03829",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "encoder = ae.encoder",
   "id": "d19a1089c712cd8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ae",
   "id": "5d7a7570ef7784e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "encoder.__dict__",
   "id": "3688fe259e049fba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "normal_audio = torchaudio.load('../normal_test.wav')[0].unsqueeze(0)",
   "id": "6ed13a5842127aca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "normal_audio.to('cpu')\n",
    "ae.to('cpu')"
   ],
   "id": "b5b748341512ecb6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "latents, latent_info = ae.encode(normal_audio, return_info=True)",
   "id": "f33ee1af4e2bb003",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "latents.shape",
   "id": "d9266e8e832b163a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "latent_info",
   "id": "cadd0c0170d92df3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from aeiou import viz",
   "id": "d3289c24c35c7c97",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "viz.tokens_spectrogram_image(latents)",
   "id": "efe1fd390c470657",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# output: (channels, samples) float32 on CPU, normalized safely\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(normal_audio.squeeze(0).numpy(), rate=44100))"
   ],
   "id": "9e41e6142a13a318",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "viz.playable_spectrogram(normal_audio.squeeze(0), sample_rate=41000, output_type=\"live\")",
   "id": "af082829f1052fd4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
