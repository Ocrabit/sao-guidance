{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchcrepe\n",
    "from einops import rearrange\n",
    "from stable_audio_tools import get_pretrained_model\n",
    "from stable_audio_tools.inference.generation import generate_diffusion_cond\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import pretty_midi\n",
    "import sounddevice as sd\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "import soundfile as sf\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "# Custom Helpers\n",
    "from audio_helpers import get_github_audio, plot_pitch, plot_pitch_comparison, animate_pitch_arrays, create_gradio_interface\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"Using {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6ddef3aa76ce02",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.enable_grad()\n",
    "def calculate_pitch(audio, sample_rate, model_type='tiny'):\n",
    "    # Compute pitch\n",
    "    if isinstance(audio, np.ndarray):\n",
    "        audio = torch.tensor(audio, device=device, dtype=torch.float32)\n",
    "\n",
    "    if audio.ndim > 1:\n",
    "        audio = audio.to(device=device, dtype=torch.float32).mean(dim=0, keepdim=True)\n",
    "\n",
    "    hop = int(sample_rate / 500.)  # 5 ms hop\n",
    "    pitch, periodicity = torchcrepe.predict(audio, sample_rate, hop_length=hop, fmin=50, fmax=550,\n",
    "                            model=model_type, # or 'full'\n",
    "                            batch_size=4096, device=device, return_periodicity=True, decoder=torchcrepe.decode.soft_argmax, differentiable=True)\n",
    "    # Clean up pitch\n",
    "    win_l = 3\n",
    "    periodicity = torchcrepe.filter.median(periodicity, win_l)\n",
    "    # periodicity = torchcrepe.threshold.Silence(-60.)(periodicity, audio, sample_rate, hop)\n",
    "    pitch = torchcrepe.threshold.At(.5)(pitch, periodicity)\n",
    "    pitch = torchcrepe.filter.mean(pitch, win_l)\n",
    "    return pitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d068395ba8477eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model | Stable Audio Open Small\n",
    "# `https://huggingface.co/stabilityai/stable-audio-open-small`\n",
    "model, model_config = get_pretrained_model(\"stabilityai/stable-audio-open-small\")\n",
    "sample_rate = model_config[\"sample_rate\"]\n",
    "sample_size = model_config[\"sample_size\"]\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efcc4f59026ee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you don't have the audio file download it\n",
    "get_github_audio(\"https://github.com/pdx-cs-sound/wavs/raw/refs/heads/main/gc.wav\")\n",
    "\n",
    "target_audio, target_sr = torchaudio.load('../data/audio/gc.wav')\n",
    "if sample_rate != target_sr: # Resample to model rate\n",
    "    resampler = torchaudio.transforms.Resample(sample_rate, target_sr)\n",
    "    target_audio = resampler(target_audio)\n",
    "\n",
    "# Reduce to this really specific time that stable audio open small has\n",
    "time_sec = 11.888616780045352\n",
    "target_audio = target_audio[:, :int(time_sec * sample_rate)]\n",
    "target_pitch = calculate_pitch(target_audio, sample_rate, 'tiny')\n",
    "\n",
    "print(f\"Target length is: {target_audio.shape[1] / sample_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0f40f7-33ab-43a4-9109-3c4b03338a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets plot the target_pitch\n",
    "plot_pitch(target_pitch, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df465561-e356-4241-98f5-437170f7e0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def pitch_callback(autoencoder, target_pitch, base_step_scale, alpha, inner_strength, in_dict):\n",
    "    x, denoised, t = in_dict['x'], in_dict['denoised'], in_dict['t'].detach().float()\n",
    "\n",
    "    step = in_dict['i'] # grab step\n",
    "    pbar = in_dict.get('pbar', None) # I updated stable-audio-tools to return this in in_dict\n",
    "\n",
    "    # PnP-Flow schedule: (1 - t)^alpha\n",
    "    time_weight = (1.0 - t) ** alpha\n",
    "    step_scale  = base_step_scale * time_weight\n",
    "    inner_strength = inner_strength\n",
    "    # print(f\"time_weight = {float(time_weight):.4f}, step_scale = {float(step_scale):.4f}, inner_strength = {float(inner_strength):.2f}\")\n",
    "    \n",
    "    with torch.enable_grad():\n",
    "        denoised.requires_grad_(True)\n",
    "        # print(f\"t = {in_dict['t']:.3f}, denoised..shape, .requires_grad = {denoised.shape},, {denoised.requires_grad}\")\n",
    "\n",
    "        pred_audio = autoencoder.decoder(denoised.half()).float() # convert to float 32\n",
    "\n",
    "        shape_save = pred_audio.shape[-1]\n",
    "        kernel_size = 64\n",
    "        pred_audio = torch.nn.functional.avg_pool1d(\n",
    "            pred_audio, kernel_size=kernel_size, stride=1, padding=kernel_size//2\n",
    "        )\n",
    "        pred_audio = pred_audio[..., :shape_save]\n",
    "\n",
    "        # print(f\"Decoder Audio \\nshape: {audio.shape} \\ndtype: {audio.dtype}\")\n",
    "        pred_audio = rearrange(pred_audio, \"b d n -> d (b n)\")\n",
    "        # print(\"Generated Audio shape:\", audio.shape, f\"Generated Audio length: {(audio.shape[1]/sample_rate):.2f}\")\n",
    "\n",
    "        # Display audio at each step\n",
    "        diplay_audio = pred_audio.div(torch.max(torch.abs(pred_audio))).clamp(-1, 1).mul(32767).to(torch.int16).cpu() \n",
    "        # how stable audio converted it\n",
    "        # display(Audio(diplay_audio.numpy(), rate=sample_rate))\n",
    "\n",
    "        # Compute pitch\n",
    "        pred_audio = pred_audio.mean(dim=0, keepdim=True)\n",
    "        pitch = calculate_pitch(pred_audio, sample_rate, 'tiny')\n",
    "\n",
    "        loss_fn = MSELoss()\n",
    "        loss = loss_fn(torch.nan_to_num(pitch, nan=0.),\n",
    "                        torch.nan_to_num(target_pitch, nan=0.))\n",
    "        # print(f\"diff scale {torch.nan_to_num(pitch, nan=0.).std()}\")\n",
    "        # loss = mse_loss + 0.01 * (1.0 / (torch.nan_to_num(pitch, nan=0.).std() + 1e-6))\n",
    "\n",
    "        # Graph both pitch\n",
    "        if step % 10 == 0:  # lets save every ten\n",
    "            pitch_array.append(pitch.detach().cpu().numpy()) # make sure to use this in jupter for external context\n",
    "            audio_array.append(diplay_audio.detach().cpu().numpy())\n",
    "            # plot_pitch_comparison(pitch, target_pitch, sample_rate, overlay=True)\n",
    "\n",
    "        grad_x = torch.autograd.grad(loss, denoised, grad_outputs=torch.ones_like(loss), retain_graph=False, allow_unused=True)[0]\n",
    "        # d_denoised = -step_scale * grad_x\n",
    "\n",
    "        if grad_x is None:\n",
    "            print(\"Warning: grad_x is None (no gradient path from pitch to denoised)\")\n",
    "            return\n",
    "\n",
    "        with torch.no_grad():\n",
    "            grad_norm = grad_x.norm()\n",
    "            if torch.isfinite(grad_norm) and grad_norm > 0.1:\n",
    "                grad_x = grad_x * (0.1 / grad_norm)\n",
    "            # denoised -= step_scale * grad_x\n",
    "            denoised.add_(-inner_strength * step_scale * grad_x)\n",
    "            denoised.requires_grad_(False)\n",
    "\n",
    "            # print(f\"loss={loss.item():.6f}\")\n",
    "            if pbar is not None:\n",
    "                # pbar.set_postfix({'one': 1, 'two': 2})\n",
    "                # print(pbar.postfix)\n",
    "                pbar.set_postfix({**({k.strip(): v.strip() for k, v in (item.split('=') for item in pbar.postfix.split(','))} \n",
    "                                     if pbar.postfix else {}),  'loss': f\"{loss.item():.2f}\"})\n",
    "\n",
    "# def cfg_scheduler(cfg_scale, step, p=.4, k =.5):\n",
    "#     # return min(max(0.0, cfg_scale), (math.log(step) ** 2)/100)\n",
    "#     return cfg_scale * math.exp(-k * (step / math.exp(p)))\n",
    "\n",
    "def cfg_scheduler(cfg_scale, step, cutoff=5, middle_descend=2):\n",
    "    if step < cutoff:\n",
    "        return cfg_scale\n",
    "    elif step < cutoff + middle_descend:\n",
    "        return cfg_scale * (middle_descend - (step - cutoff)) / middle_descend\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d2fd5a-0ab1-41ef-9f0b-66e062814d2e",
   "metadata": {},
   "source": [
    "##### Random scheduler I created on desmos\n",
    "##### Note didnt even end up using this\n",
    "$$\n",
    "\\Large\n",
    "\\text{CFG}(step) = c \\cdot e^{\\left(-k \\cdot \\left(\\frac{step}{e^{p}}\\right)\\right)}\n",
    "$$\n",
    "- **\\( \\text{CFG}(step) \\)** — the scheduled classifier-free guidance scale at this sampling step  \n",
    "- **\\( c \\)** — the initial CFG scale (your starting guidance strength)  \n",
    "- **\\( step \\)** — the current diffusion iteration (1, 2, 3, …)  \n",
    "- **\\( k \\)** — a decay constant controlling how quickly CFG falls toward zero  \n",
    "- **\\( p \\)** — a shaping parameter that adjusts how sharply the decay curve bends  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6069e2d26586838",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditioning = [{\n",
    "    # \"prompt\": \"nylon guitar country sound\",  # This prompt is quite bad on small, but small does work\n",
    "    \"prompt\": \"my saxophone cried a whale on a sunday\",  # This prompt is quite bad on small, but small does work\n",
    "    \"seconds_total\": time_sec\n",
    "}]\n",
    "pitch_array = []\n",
    "audio_array = []\n",
    "\n",
    "autoencoder = model._modules['pretransform']._modules.get(\"model\")\n",
    "# Params\n",
    "base_step_scale = 5\n",
    "alpha = 10\n",
    "inner_strength = 50\n",
    "callback_wrapper = partial(pitch_callback, autoencoder, target_pitch, base_step_scale, alpha, inner_strength)\n",
    "cfg_scheduler_wrapper = partial(cfg_scheduler, cutoff=20, middle_descend=2)\n",
    "\n",
    "# callback_wrapper = partial(pitch_guidance_callback, autoencoder, target_pitch, 50)\n",
    "\n",
    "# Generate stereo audio\n",
    "output = generate_diffusion_cond(\n",
    "    model,\n",
    "    # Marco's Notes:\n",
    "    # 7 steps works good for sao small, higher than that gets scary\n",
    "    # If using normal sao higher steps is usually pretty good.\n",
    "    conditioning=conditioning,\n",
    "    steps=50,\n",
    "    cfg_scale=10, # Config of 1 often good for small, higher works on normal\n",
    "    sample_size=sample_size,\n",
    "    sigma_min=10,\n",
    "    sigma_max=300,\n",
    "    # sampler_type=\"dpmpp-3m-sde\",  # Use this for normal open\n",
    "    sampler_type=\"pingpong\",  # Use this for small\n",
    "    device=device,\n",
    "    callback=callback_wrapper,\n",
    "    cfg_scheduler=cfg_scheduler_wrapper\n",
    ")\n",
    "\n",
    "# Rearrange audio batch to a single sequence\n",
    "output = rearrange(output, \"b d n -> d (b n)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5161cca3e0618",
   "metadata": {},
   "source": [
    "##### Now you can hear the final audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aba34dc2899a136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peak normalize, convert to int16\n",
    "cleaned_output = output.to(torch.float32).div(torch.max(torch.abs(output))).clamp(-1, 1).mul(32767).to(torch.int16).cpu()\n",
    "\n",
    "# Clip length\n",
    "#clipped_output = cleaned_output[..., :int(sample_rate * total_seconds)]\n",
    "\n",
    "Audio(cleaned_output.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c5385d-769a-483a-938c-4a4dcb332685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up\n",
    "import gc\n",
    "print(gc.collect())\n",
    "import torch\n",
    "print(torch.cuda.empty_cache())\n",
    "print(torch.cuda.synchronize())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfded6953c3365f",
   "metadata": {},
   "source": [
    "Sounds like garbage right? (well thats if you took over like 10 steps)\n",
    "Instead lets run gradio and try to see what some of the audio looked like during the journey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba91c788b91ab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = create_gradio_interface(pitch_array, target_pitch, sample_rate, audio_array, target_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7179cac50ab76a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.launch(server_name=\"0.0.0.0\", server_port=7860, inline=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cce22b0-d387-4a3d-99a8-e7b822da6841",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780262049d5432f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to close any still open ones\n",
    "import gradio as gr\n",
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85941118e5d76363",
   "metadata": {},
   "source": [
    "You can also view just the pitch frames overlaid with the `target_pitch` and then select the audio frame in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b0f25d-7b4b-4144-a48b-afe965a0b999",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pitch(target_pitch, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0eaaa7ecc69d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "animate_pitch_arrays(pitch_array, sample_rate, target_pitch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b854e7b2a4fc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_number = 4\n",
    "Audio(audio_array[frame_number], rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93dc89cd881b1f6",
   "metadata": {},
   "source": [
    "If you want to compare to the original audio run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9788093827cb5d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also display original audio\n",
    "Audio(target_audio.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed5eb0873476838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20e37495f50f9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def pitch_guidance_callback(autoencoder, target_pitch, strength, in_dict):\n",
    "    \"Main guidance routine using chroma features\"\n",
    "    t, x, denoised = in_dict['t'], in_dict['x'], in_dict['denoised']\n",
    "    if target_pitch is None:\n",
    "        print(f\"t = {t:.3f}: target_pitch is None. Skipping pitch guidance callback\")\n",
    "        return\n",
    "    inner_strength = strength / 10  # or /5, adjust based on testing\n",
    "    print('--')\n",
    "    with torch.enable_grad():\n",
    "        for step in range(10):\n",
    "            denoised.requires_grad_(True)  # Enable grad on denoised itself\n",
    "            pred_audio = autoencoder.decoder(denoised.half())\n",
    "            shape_save = pred_audio.shape[-1]\n",
    "            kernel_size = 64\n",
    "            pred_audio = torch.nn.functional.avg_pool1d(\n",
    "                pred_audio, kernel_size=kernel_size, stride=1, padding=kernel_size//2\n",
    "            )\n",
    "            pred_audio = pred_audio[..., :shape_save]\n",
    "            #print(\"pred_audio.shape =\",pred_audio.shape)\n",
    "            # pred_audio = torchaudio.functional.lowpass_biquad(\n",
    "            #     pred_audio, sample_rate=sample_rate, cutoff_freq=4000)\n",
    "            pred_audio = rearrange(pred_audio, \"b d n -> d (b n)\")\n",
    "            pred_pitch = calculate_pitch(pred_audio, sample_rate)\n",
    "\n",
    "            loss = F.mse_loss(\n",
    "                torch.nan_to_num(pred_pitch, nan=0),\n",
    "                torch.nan_to_num(target_pitch, nan=0)\n",
    "                )\n",
    "            grad = torch.autograd.grad(loss, denoised)[0]\n",
    "            #gamma_t = (1 - t)\n",
    "            gamma_t = 0.3 + 0.7 * (1 - t)  # Goes from 0.3 → 1.0\n",
    "\n",
    "            # Clip and update\n",
    "            with torch.no_grad():\n",
    "                if grad.norm() > 0.1:\n",
    "                    grad = grad * (0.1 / grad.norm())\n",
    "                denoised -= inner_strength * gamma_t * grad\n",
    "                denoised.requires_grad_(False)\n",
    "\n",
    "            print(f\"  Inner step {step}: loss={loss.item():.6f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
