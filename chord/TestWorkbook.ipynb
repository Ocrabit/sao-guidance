{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bc5ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\simeo\\VSCodeProjects\\StableAudioProject\\sao-guidance\\.venv\\lib\\site-packages\\clip\\clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import packaging\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from einops import rearrange\n",
    "from stable_audio_tools import get_pretrained_model\n",
    "from stable_audio_tools.inference.generation import generate_diffusion_cond\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dea1a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'flash_attn'\n",
      "flash_attn not installed, disabling Flash Attention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\simeo\\VSCodeProjects\\StableAudioProject\\sao-guidance\\.venv\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    }
   ],
   "source": [
    "# Download model\n",
    "model, model_config = get_pretrained_model(\"stabilityai/stable-audio-open-small\")\n",
    "sample_rate = model_config[\"sample_rate\"]\n",
    "sample_size = model_config[\"sample_size\"]\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d161d7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import typing as tp\n",
    "from librosa import filters\n",
    "import matplotlib as plt\n",
    "import einops\n",
    "import numpy as np\n",
    "import julius\n",
    "import torchmetrics\n",
    "\n",
    "class ChromaExtractor(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Chroma extraction and quantization.\n",
    "\n",
    "    Args:\n",
    "        sample_rate (int): Sample rate for the chroma extraction.\n",
    "        n_chroma (int): Number of chroma bins for the chroma extraction.\n",
    "        radix2_exp (int): Size of stft window for the chroma extraction (power of 2, e.g. 12 -> 2^12).\n",
    "        nfft (int, optional): Number of FFT.\n",
    "        winlen (int, optional): Window length.\n",
    "        winhop (int, optional): Window hop size.\n",
    "        argmax (bool, optional): Whether to use argmax. Defaults to False.\n",
    "        norm (float, optional): Norm for chroma normalization. Defaults to inf.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sample_rate: int, n_chroma: int = 12, radix2_exp: int = 12, nfft: tp.Optional[int] = None,\n",
    "                 winlen: tp.Optional[int] = None, winhop: tp.Optional[int] = None, argmax: bool = False,\n",
    "                 norm: float = torch.inf):\n",
    "        super().__init__()\n",
    "        self.winlen = winlen or 2 ** radix2_exp\n",
    "        self.nfft = nfft or self.winlen\n",
    "        self.winhop = winhop or (self.winlen // 4)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_chroma = n_chroma\n",
    "        self.norm = norm\n",
    "        self.argmax = argmax\n",
    "        self.register_buffer('fbanks', torch.from_numpy(filters.chroma(sr=sample_rate, n_fft=self.nfft, tuning=0,\n",
    "                                                                       n_chroma=self.n_chroma)), persistent=False)\n",
    "        self.spec = torchaudio.transforms.Spectrogram(n_fft=self.nfft, win_length=self.winlen,\n",
    "                                                      hop_length=self.winhop, power=2, center=True,\n",
    "                                                      pad=0, normalized=True)\n",
    "\n",
    "    def forward(self, wav: torch.Tensor) -> torch.Tensor:\n",
    "        T = wav.shape[-1]\n",
    "        # in case we are getting a wav that was dropped out (nullified)\n",
    "        # from the conditioner, make sure wav length is no less that nfft\n",
    "        if T < self.nfft:\n",
    "            pad = self.nfft - T\n",
    "            r = 0 if pad % 2 == 0 else 1\n",
    "            wav = torch.nn.functional.pad(wav, (pad // 2, pad // 2 + r), 'constant', 0)\n",
    "            assert wav.shape[-1] == self.nfft, f\"expected len {self.nfft} but got {wav.shape[-1]}\"\n",
    "\n",
    "        spec = self.spec(wav).squeeze(1)\n",
    "        raw_chroma = torch.einsum('cf,...ft->...ct', self.fbanks, spec)\n",
    "        norm_chroma = torch.nn.functional.normalize(raw_chroma, p=self.norm, dim=-2, eps=1e-6)\n",
    "        norm_chroma = einops.rearrange(norm_chroma, 'b d t -> b t d')\n",
    "\n",
    "        if self.argmax:\n",
    "            idx = norm_chroma.argmax(-1, keepdim=True)\n",
    "            norm_chroma[:] = 0\n",
    "            norm_chroma.scatter_(dim=-1, index=idx, value=1)\n",
    "\n",
    "        return norm_chroma\n",
    "    \n",
    "class ChromaCosineSimilarityMetric(torchmetrics.Metric):\n",
    "    \n",
    "    \"\"\"\n",
    "    Chroma cosine similarity metric.\n",
    "\n",
    "        This metric extracts a chromagram for a reference waveform and\n",
    "        a generated waveform and compares each frame using the cosine similarity\n",
    "        function. The output is the mean cosine similarity.\n",
    "\n",
    "        Args:\n",
    "            sample_rate (int): Sample rate used by the chroma extractor.\n",
    "            n_chroma (int): Number of chroma used by the chroma extractor.\n",
    "            radix2_exp (int): Exponent for the chroma extractor.\n",
    "            argmax (bool): Whether the chroma extractor uses argmax.\n",
    "            eps (float): Epsilon for cosine similarity computation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sample_rate: int, n_chroma: int, radix2_exp: int, argmax: bool, eps: float = 1e-8):\n",
    "        super().__init__()\n",
    "        self.chroma_sample_rate = sample_rate\n",
    "        self.n_chroma = n_chroma\n",
    "        self.eps = eps\n",
    "        self.chroma_extractor = ChromaExtractor(sample_rate=self.chroma_sample_rate, n_chroma=self.n_chroma,\n",
    "                                                radix2_exp=radix2_exp, argmax=argmax)\n",
    "        self.add_state(\"cosine_sum\", default=torch.tensor(0.), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"weight\", default=torch.tensor(0.), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    @torch.enable_grad()\n",
    "    def update(self, preds: torch.Tensor, targets: torch.Tensor,\n",
    "               sizes: torch.Tensor, sample_rates: torch.Tensor) -> None:\n",
    "        \"\"\"Compute cosine similarity between chromagrams and accumulate scores over the dataset.\"\"\"\n",
    "        if preds.size(0) == 0:\n",
    "            return\n",
    "\n",
    "        assert preds.shape == targets.shape, (\n",
    "            f\"Preds and target shapes mismatch: preds={preds.shape}, targets={targets.shape}\")\n",
    "        assert preds.size(0) == sizes.size(0), (\n",
    "            f\"Number of items in preds ({preds.shape}) mismatch \",\n",
    "            f\"with sizes ({sizes.shape})\")\n",
    "        assert preds.size(0) == sample_rates.size(0), (\n",
    "            f\"Number of items in preds ({preds.shape}) mismatch \",\n",
    "            f\"with sample_rates ({sample_rates.shape})\")\n",
    "        assert torch.all(sample_rates == sample_rates[0].item()), \"All sample rates are not the same in the batch\"\n",
    "\n",
    "        device = self.weight.device\n",
    "        preds, targets = preds.to(device), targets.to(device)  # type: ignore\n",
    "        print(\"preds.requires_grad, targets.requires_grad =\",preds.requires_grad, targets.requires_grad)\n",
    "        # sample_rate = sample_rates[0].item()\n",
    "        # preds = convert_audio(preds, from_rate=sample_rate, to_rate=self.chroma_sample_rate, to_channels=1)\n",
    "        # targets = convert_audio(targets, from_rate=sample_rate, to_rate=self.chroma_sample_rate, to_channels=1)\n",
    "        gt_chroma = self.chroma_extractor(targets)\n",
    "        gen_chroma = self.chroma_extractor(preds)\n",
    "        chroma_lens = (sizes / self.chroma_extractor.winhop).ceil().int()\n",
    "        for i in range(len(gt_chroma)):\n",
    "            t = int(chroma_lens[i].item())\n",
    "            cosine_sim = torch.nn.functional.cosine_similarity(\n",
    "                gt_chroma[i, :t], gen_chroma[i, :t], dim=1, eps=self.eps)\n",
    "            self.cosine_sum += cosine_sim.sum(dim=0)  # type: ignore\n",
    "            self.weight += torch.tensor(t)  # type: ignore\n",
    "\n",
    "    @torch.enable_grad()\n",
    "    def compute(self) -> float:\n",
    "        \"\"\"Computes the average cosine similarty across all generated/target chromagrams pairs.\"\"\"\n",
    "        assert self.weight.item() > 0, \"Unable to compute with total number of comparisons <= 0\"  # type: ignore\n",
    "        return (self.cosine_sum / self.weight) #.item()  # type: ignore\n",
    "    \n",
    "def convert_audio(wav: torch.Tensor, from_rate: float,\n",
    "                  to_rate: float, to_channels: int) -> torch.Tensor:\n",
    "    \"\"\"Convert audio to new sample rate and number of audio channels.\"\"\"\n",
    "    wav = julius.resample_frac(wav, int(from_rate), int(to_rate))\n",
    "    wav = convert_audio_channels(wav, to_channels)\n",
    "    return wav\n",
    "\n",
    "def convert_audio_channels(wav: torch.Tensor, channels: int = 2) -> torch.Tensor:\n",
    "\n",
    "    *shape, src_channels, length = wav.shape\n",
    "    if src_channels == channels:\n",
    "        pass\n",
    "    elif channels == 1:\n",
    "        # Case 1:\n",
    "        # The caller asked 1-channel audio, and the stream has multiple\n",
    "        # channels, downmix all channels.\n",
    "        wav = wav.mean(dim=-2, keepdim=True)\n",
    "    elif src_channels == 1:\n",
    "        # Case 2:\n",
    "        # The caller asked for multiple channels, but the input file has\n",
    "        # a single channel, replicate the audio over all channels.\n",
    "        wav = wav.expand(*shape, channels, length)\n",
    "    elif src_channels >= channels:\n",
    "        # Case 3:\n",
    "        # The caller asked for multiple channels, and the input file has\n",
    "        # more channels than requested. In that case return the first channels.\n",
    "        wav = wav[..., :channels, :]\n",
    "    else:\n",
    "        # Case 4: What is a reasonable choice here?\n",
    "        raise ValueError('The audio file has less channels than requested but is not mono.')\n",
    "    return wav\n",
    "\n",
    "def plot_chromagram(chroma, sample_rate, hop_length):\n",
    "    # Get parameters\n",
    "    n_frames = chroma.shape[1]        # number of time frames\n",
    "\n",
    "    # Compute time axis (in seconds)\n",
    "    times = np.arange(n_frames) * hop_length / sample_rate\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(chroma[0].T.cpu(), \n",
    "            aspect='auto', \n",
    "            origin='lower', \n",
    "            extent=[times[0], times[-1], 0, 12])\n",
    "\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Chroma bins')\n",
    "    plt.title('Chroma Features')\n",
    "\n",
    "    # Set y-axis labels to note names\n",
    "    note_labels = ['C', 'C#', 'D', 'D#', 'E', 'F', \n",
    "                'F#', 'G', 'G#', 'A', 'A#', 'B']\n",
    "\n",
    "    plt.yticks(ticks=range(len(note_labels)), labels=note_labels)\n",
    "    plt.colorbar(label='Intensity')\n",
    "    plt.show()\n",
    "\n",
    "def chroma_guidance_callback(model, target_audio, step_scale, in_dict):\n",
    "    denoised = in_dict['denoised']  # model prediction (latent audio or waveform)\n",
    "    x = in_dict['x']\n",
    "    with torch.enable_grad():\n",
    "        x.requires_grad = True\n",
    "        denoised.requires_grad = True\n",
    "\n",
    "        print(f\"t = {in_dict['t']:.3f}, denoised..shape, .requires_grad = {denoised.shape},, {denoised.requires_grad}\") \n",
    "\n",
    "        # Convert latent -> waveform\n",
    "        autoencoder = model._modules['pretransform']._modules.get(\"model\")\n",
    "        print(f\"Denoised Shape: {denoised.shape}\")\n",
    "\n",
    "        pred_audio = autoencoder.decoder(denoised.half())\n",
    "        pred_audio = einops.rearrange(pred_audio, \"b d n -> d (b n)\")\n",
    "        print(f\"P aud Shape: {pred_audio.shape}\")\n",
    "\n",
    "        # Create chroma metric/loss\n",
    "        chroma_metric = ChromaCosineSimilarityMetric(\n",
    "            sample_rate=autoencoder.sample_rate,\n",
    "            n_chroma=12,\n",
    "            radix2_exp=12,\n",
    "            argmax=False\n",
    "        )\n",
    "\n",
    "        # Fake wrappers for batch info\n",
    "        B = pred_audio.shape[0]\n",
    "        sizes = torch.tensor([pred_audio.shape[-1]] * B, device=pred_audio.device)\n",
    "        sample_rates = torch.tensor([44100] * B, device=pred_audio.device)\n",
    "\n",
    "        # Compute similarity\n",
    "        chroma_metric.update(pred_audio, target_audio, sizes, sample_rates)\n",
    "        similarity = chroma_metric.compute()\n",
    "\n",
    "        print(\"chromagram.shape =\",pred_audio.shape,\", chromagram.requires_grad =\", pred_audio.requires_grad)\n",
    "        # plot_chromagram(pred_audio, sample_rate=autoencoder.sample_rate, winhop=ChromaCosineSimilarityMetric.chroma_extractor.winhop)\n",
    "\n",
    "        # Compute similarity\n",
    "        chroma_metric.update(pred_audio, target_audio, sizes, sample_rates)\n",
    "        similarity = chroma_metric.compute()\n",
    "\n",
    "        # Convert to loss (maximize similarity → minimize 1 - similarity)\n",
    "        chroma_loss = 1.0 - similarity\n",
    "        print(\"chroma_loss.requires_grad =\",chroma_loss.requires_grad)\n",
    "        grad_x = torch.autograd.grad(chroma_loss, denoised, grad_outputs=torch.ones_like(chroma_loss), retain_graph=False, allow_unused=True)[0]\n",
    "        #print(\"grad_x = \",grad_x)\n",
    "        d_denoised = -step_scale * grad_x\n",
    "        denoised = denoised + d_denoised\n",
    "\n",
    "        # denoised.requires_grad = False\n",
    "        # x.requires_grad = False\n",
    "        \n",
    "    # Return or backpropagate\n",
    "    in_dict['denoised'] = denoised\n",
    "    in_dict['x'] = x\n",
    "    return\n",
    "\n",
    "def trim_audio_seconds(wav: torch.Tensor, sample_rate: int, duration_s: float):\n",
    "    \"\"\"Trim (or pad) an audio tensor to the desired duration in seconds.\"\"\"\n",
    "    target_len = int(sample_rate * duration_s)\n",
    "    wav = wav[..., :target_len]  # Trim\n",
    "    return wav\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603b1cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guidance signal\n",
    "\n",
    "audio_path = f\"./Data/panflute_fast.wav\"\n",
    "\n",
    "t_aud, t_sr = torchaudio.load(audio_path)\n",
    "display(Audio(t_aud.numpy(), rate=t_sr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be1b974",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1y/9jpm_bzs7mnd88sp14tmnwwc0000gn/T/ipykernel_57178/2458772132.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  wav = torch.tensor(wav).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([2, 524288])\n",
      "44100\n",
      "1234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 1.000, denoised..shape, .requires_grad = torch.Size([1, 64, 256]),, False\n",
      "Denoised Shape: torch.Size([1, 64, 256])\n",
      "P aud Shape: torch.Size([2, 524288])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chromagram.shape = torch.Size([2, 524288]) , chromagram.requires_grad = False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ones_like(): argument 'input' (position 1) must be Tensor, not float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 23\u001b[0m\n\u001b[1;32m     17\u001b[0m conditioning \u001b[38;5;241m=\u001b[39m [{\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmono jazz saxophone solo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseconds_total\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     20\u001b[0m }]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Generate stereo audio\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_diffusion_cond\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Marco's Notes:\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 7 steps works good for sao small, higher than that gets scary\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If using normal sao higher steps is usually pretty good.\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcfg_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Config of 1 often good for small, higher works on normal\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconditioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconditioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43msigma_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43msigma_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#sampler_type=\"dpmpp-3m-sde\",  # Use this for normal open\u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampler_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpingpong\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use this for small\u001b[39;49;00m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_wrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1234\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/DLAIE/self/temp/sao-guidance/.venv/lib/python3.10/site-packages/stable_audio_tools/inference/generation.py:204\u001b[0m, in \u001b[0;36mgenerate_diffusion_cond\u001b[0;34m(model, steps, cfg_scale, conditioning, conditioning_tensors, negative_conditioning, negative_conditioning_tensors, batch_size, sample_size, sample_rate, seed, device, init_audio, init_noise_level, return_latents, **sampler_kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrho\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sampler_kwargs:\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m sampler_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrho\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 204\u001b[0m     sampled \u001b[38;5;241m=\u001b[39m \u001b[43msample_rf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_audio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msampler_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconditioning_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnegative_conditioning_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdist_shift\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdist_shift\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrescale_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# v-diffusion: \u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m#sampled = sample(model.model, noise, steps, 0, **conditioning_tensors, embedding_scale=cfg_scale)\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m noise\n",
      "File \u001b[0;32m~/Projects/DLAIE/self/temp/sao-guidance/.venv/lib/python3.10/site-packages/stable_audio_tools/inference/sampling.py:457\u001b[0m, in \u001b[0;36msample_rf\u001b[0;34m(model_fn, noise, init_data, steps, sampler_type, sigma_max, device, callback, cond_fn, **extra_args)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sample_flow_dpmpp(model_fn, x, sigmas\u001b[38;5;241m=\u001b[39mt, sigma_max\u001b[38;5;241m=\u001b[39msigma_max, callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_args)\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sampler_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpingpong\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msample_flow_pingpong\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigmas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigma_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/DLAIE/self/temp/sao-guidance/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/DLAIE/self/temp/sao-guidance/.venv/lib/python3.10/site-packages/stable_audio_tools/inference/sampling.py:244\u001b[0m, in \u001b[0;36msample_flow_pingpong\u001b[0;34m(model, x, steps, sigma_max, sigmas, callback, dist_shift, **extra_args)\u001b[0m\n\u001b[1;32m    242\u001b[0m denoised \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m-\u001b[39m t[i] \u001b[38;5;241m*\u001b[39m model(x, t[i] \u001b[38;5;241m*\u001b[39m ts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_args)\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 244\u001b[0m     \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msigma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msigma_hat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdenoised\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenoised\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m t_next \u001b[38;5;241m=\u001b[39m t[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    247\u001b[0m x \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mt_next) \u001b[38;5;241m*\u001b[39m denoised \u001b[38;5;241m+\u001b[39m t_next \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(x)\n",
      "Cell \u001b[0;32mIn[33], line 229\u001b[0m, in \u001b[0;36mchroma_guidance_callback\u001b[0;34m(model, target_audio, step_scale, in_dict)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# Convert to loss (maximize similarity → minimize 1 - similarity)\u001b[39;00m\n\u001b[1;32m    228\u001b[0m chroma_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m similarity\n\u001b[0;32m--> 229\u001b[0m grad_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(chroma_loss, x, grad_outputs\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchroma_loss\u001b[49m\u001b[43m)\u001b[49m, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    230\u001b[0m d_denoised \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mstep_scale \u001b[38;5;241m*\u001b[39m grad_x\n\u001b[1;32m    231\u001b[0m denoised \u001b[38;5;241m=\u001b[39m denoised \u001b[38;5;241m+\u001b[39m d_denoised\n",
      "\u001b[0;31mTypeError\u001b[0m: ones_like(): argument 'input' (position 1) must be Tensor, not float"
     ]
    }
   ],
   "source": [
    "from functools import partial \n",
    "\n",
    "wav = trim_audio_seconds(t_aud, sample_rate=t_sr, duration_s=11.888616780045352)\n",
    "callback_wrapper = partial(chroma_guidance_callback, model, wav, 0.1)\n",
    "\n",
    "\n",
    "conditioning = [{\n",
    "    \"prompt\": \"mono jazz saxophone solo\",\n",
    "    \"seconds_total\": 10\n",
    "}]\n",
    "\n",
    "# Generate stereo audio\n",
    "output = generate_diffusion_cond(\n",
    "    model,\n",
    "    # Marco's Notes:\n",
    "    # 7 steps works good for sao small, higher than that gets scary\n",
    "    # If using normal sao higher steps is usually pretty good.\n",
    "    steps=7,\n",
    "    cfg_scale=1, # Config of 1 often good for small, higher works on normal\n",
    "    conditioning=conditioning,\n",
    "    sample_size=sample_size,\n",
    "    sigma_min=.3,\n",
    "    sigma_max=500,\n",
    "    #sampler_type=\"dpmpp-3m-sde\",  # Use this for normal open\n",
    "    sampler_type=\"pingpong\",  # Use this for small\n",
    "    device=device,\n",
    "    callback=callback_wrapper,\n",
    "    seed=1234,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5c1767",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = rearrange(output, \"b d n -> d (b n)\")\n",
    "\n",
    "# Peak normalize, convert to int16\n",
    "cleaned_output = output.to(torch.float32).div(torch.max(torch.abs(output))).clamp(-1, 1).mul(32767).to(torch.int16).cpu()\n",
    "\n",
    "# Clip length\n",
    "#clipped_output = cleaned_output[..., :int(sample_rate * total_seconds)]\n",
    "\n",
    "display(Audio(cleaned_output.numpy(), rate=sample_rate))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sao (3.10.19)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
